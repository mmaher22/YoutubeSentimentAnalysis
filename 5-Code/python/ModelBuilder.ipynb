{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "import csv\n",
    "from nltk.stem.isri import ISRIStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download(\"movie_reviews\", 'C:\\\\Users\\\\MohamedMaher\\\\Anaconda3\\\\nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_formatted = []\n",
    "wordsen = []\n",
    "wordsar = []\n",
    "clas = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Adding English Patterns:  2000\n"
     ]
    }
   ],
   "source": [
    "#English Dataset\n",
    "from stemming.porter2 import stem\n",
    "for cat in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(cat):\n",
    "        if cat == 'pos':\n",
    "            clas = 1\n",
    "        elif cat == 'neg':\n",
    "            clas = -1\n",
    "        else:\n",
    "            clas = 0\n",
    "        tmp = movie_reviews.words(fileid)\n",
    "        dataset_formatted.append([tmp, clas])\n",
    "\n",
    "for word in movie_reviews.words():\n",
    "    word = stem(word)\n",
    "    wordsen.extend([word.lower()])\n",
    "\n",
    "print('After Adding English Patterns: ', len(dataset_formatted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Adding Arabic Patterns:  10054\n"
     ]
    }
   ],
   "source": [
    "#Arabic Dataset\n",
    "'''\n",
    "st = ISRIStemmer()\n",
    "with open('ArabicDataset2.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    dataset = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in dataset:\n",
    "        clas = row[1]\n",
    "        row = row[0].split()\n",
    "        for r in row:\n",
    "                #print('original: ', r)\n",
    "                #stem = ArListem.light_stem(r)\n",
    "                #print('stemmed: ',stem)\n",
    "                r = st.stem(r)\n",
    "                wordsar.extend([r])\n",
    "        dataset_formatted.append([row, clas])\n",
    "        \n",
    "random.shuffle(dataset_formatted) #Shuffle dataset to avoid biasing towards certain class\n",
    "print('After Adding Arabic Patterns: ', len(dataset_formatted))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_FD = nltk.FreqDist(wordsen) #take worden frequency\n",
    "word_featuresen = list(words_FD.keys())[:10000] #take most frequent 10k words\n",
    "\n",
    "#words_FD = nltk.FreqDist(wordsar) #take wordar frequency\n",
    "#word_featuresar = list(words_FD.keys())[:10000] #take most frequent 10k words\n",
    "\n",
    "word_features =  word_featuresen\n",
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'coupl',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'parti',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accid',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guy',\n",
       " 'die',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continu',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " 'has',\n",
       " 'nightmar',\n",
       " 'what',\n",
       " \"'\",\n",
       " 's',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'movi',\n",
       " '\"',\n",
       " 'sorta',\n",
       " 'find',\n",
       " 'out',\n",
       " 'critiqu',\n",
       " 'mind',\n",
       " '-',\n",
       " 'fuck',\n",
       " 'for',\n",
       " 'generat',\n",
       " 'that',\n",
       " 'touch',\n",
       " 'on',\n",
       " 'veri',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'present',\n",
       " 'it',\n",
       " 'bad',\n",
       " 'packag',\n",
       " 'which',\n",
       " 'is',\n",
       " 'make',\n",
       " 'this',\n",
       " 'review',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'write',\n",
       " 'sinc',\n",
       " 'i',\n",
       " 'general',\n",
       " 'applaud',\n",
       " 'film',\n",
       " 'attempt',\n",
       " 'break',\n",
       " 'mold',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'way',\n",
       " 'all',\n",
       " 'type',\n",
       " 'these',\n",
       " 'folk',\n",
       " 'just',\n",
       " 'didn',\n",
       " 't',\n",
       " 'snag',\n",
       " 'correct',\n",
       " 'seem',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'pretti',\n",
       " 'neat',\n",
       " 'concept',\n",
       " 'execut',\n",
       " 'terribl',\n",
       " 'so',\n",
       " 'problem',\n",
       " 'well',\n",
       " 'main',\n",
       " 'simpli',\n",
       " 'too',\n",
       " 'jumbl',\n",
       " 'start',\n",
       " 'off',\n",
       " 'normal',\n",
       " 'downshift',\n",
       " 'fantasi',\n",
       " 'world',\n",
       " 'you',\n",
       " 'as',\n",
       " 'audienc',\n",
       " 'member',\n",
       " 'no',\n",
       " 'dream',\n",
       " 'charact',\n",
       " 'come',\n",
       " 'back',\n",
       " 'from',\n",
       " 'dead',\n",
       " 'other',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'strang',\n",
       " 'apparit',\n",
       " 'disappear',\n",
       " 'looooot',\n",
       " 'chase',\n",
       " 'scene',\n",
       " 'ton',\n",
       " 'weird',\n",
       " 'thing',\n",
       " 'happen',\n",
       " 'most',\n",
       " 'not',\n",
       " 'explain',\n",
       " 'now',\n",
       " 'person',\n",
       " 'don',\n",
       " 'tri',\n",
       " 'unravel',\n",
       " 'everi',\n",
       " 'when',\n",
       " 'doe',\n",
       " 'give',\n",
       " 'me',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'again',\n",
       " 'kind',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'while',\n",
       " 'biggest',\n",
       " 'obvious',\n",
       " 'got',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'hide',\n",
       " 'want',\n",
       " 'complet',\n",
       " 'until',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minut',\n",
       " 'do',\n",
       " 'entertain',\n",
       " 'thrill',\n",
       " 'or',\n",
       " 'engag',\n",
       " 'meantim',\n",
       " 'realli',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'arrow',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'flick',\n",
       " 'we',\n",
       " 'actual',\n",
       " 'figur',\n",
       " 'by',\n",
       " 'half',\n",
       " 'point',\n",
       " 'did',\n",
       " 'littl',\n",
       " 'bit',\n",
       " 'sens',\n",
       " 'still',\n",
       " 'more',\n",
       " 'guess',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'should',\n",
       " 'alway',\n",
       " 'sure',\n",
       " 'befor',\n",
       " 'given',\n",
       " 'password',\n",
       " 'enter',\n",
       " 'understand',\n",
       " 'mean',\n",
       " 'show',\n",
       " 'melissa',\n",
       " 'sagemil',\n",
       " 'run',\n",
       " 'away',\n",
       " 'vision',\n",
       " 'about',\n",
       " '20',\n",
       " 'throughout',\n",
       " 'plain',\n",
       " 'lazi',\n",
       " '!',\n",
       " 'okay',\n",
       " 'peopl',\n",
       " 'know',\n",
       " 'need',\n",
       " 'how',\n",
       " 'us',\n",
       " 'differ',\n",
       " 'offer',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'down',\n",
       " 'appar',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'director',\n",
       " 'chop',\n",
       " 'themselv',\n",
       " 'might',\n",
       " 've',\n",
       " 'been',\n",
       " 'decent',\n",
       " 'here',\n",
       " 'somewher',\n",
       " 'suit',\n",
       " 'decid',\n",
       " 'turn',\n",
       " 'music',\n",
       " 'video',\n",
       " 'edg',\n",
       " 'would',\n",
       " 'actor',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'be',\n",
       " 'play',\n",
       " 'exact',\n",
       " 'he',\n",
       " 'american',\n",
       " 'beauti',\n",
       " 'onli',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " 'my',\n",
       " 'kudo',\n",
       " 'hold',\n",
       " 'own',\n",
       " 'entir',\n",
       " 'feel',\n",
       " 'overal',\n",
       " 'doesn',\n",
       " 'stick',\n",
       " 'becaus',\n",
       " 'confus',\n",
       " 'rare',\n",
       " 'excit',\n",
       " 'redund',\n",
       " 'runtim',\n",
       " 'despit',\n",
       " 'end',\n",
       " 'explan',\n",
       " 'crazi',\n",
       " 'came',\n",
       " 'oh',\n",
       " 'horror',\n",
       " 'slasher',\n",
       " 'someon',\n",
       " 'assum',\n",
       " 'genr',\n",
       " 'hot',\n",
       " 'kid',\n",
       " 'also',\n",
       " 'wrap',\n",
       " 'product',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'sit',\n",
       " 'shelv',\n",
       " 'ever',\n",
       " 'whatev',\n",
       " 'skip',\n",
       " 'where',\n",
       " 'joblo',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '7',\n",
       " '/',\n",
       " '10',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " 'crow',\n",
       " '9',\n",
       " 'salvat',\n",
       " '4',\n",
       " 'stir',\n",
       " 'echo',\n",
       " '8',\n",
       " 'happi',\n",
       " 'bastard',\n",
       " 'quick',\n",
       " 'damn',\n",
       " 'y2k',\n",
       " 'bug',\n",
       " 'star',\n",
       " 'jami',\n",
       " 'lee',\n",
       " 'curti',\n",
       " 'anoth',\n",
       " 'baldwin',\n",
       " 'brother',\n",
       " 'william',\n",
       " 'time',\n",
       " 'stori',\n",
       " 'regard',\n",
       " 'crew',\n",
       " 'tugboat',\n",
       " 'across',\n",
       " 'desert',\n",
       " 'russian',\n",
       " 'tech',\n",
       " 'ship',\n",
       " 'kick',\n",
       " 'power',\n",
       " 'within',\n",
       " 'gore',\n",
       " 'bring',\n",
       " 'few',\n",
       " 'action',\n",
       " 'sequenc',\n",
       " 'virus',\n",
       " 'empti',\n",
       " 'flash',\n",
       " 'substanc',\n",
       " 'whi',\n",
       " 'was',\n",
       " 'middl',\n",
       " 'nowher',\n",
       " 'origin',\n",
       " 'pink',\n",
       " 'flashi',\n",
       " 'hit',\n",
       " 'mir',\n",
       " 'cours',\n",
       " 'donald',\n",
       " 'sutherland',\n",
       " 'stumbl',\n",
       " 'around',\n",
       " 'drunken',\n",
       " 'hey',\n",
       " 'let',\n",
       " 'some',\n",
       " 'robot',\n",
       " 'act',\n",
       " 'below',\n",
       " 'averag',\n",
       " 're',\n",
       " 'work',\n",
       " 'halloween',\n",
       " 'h20',\n",
       " 'wast',\n",
       " 'real',\n",
       " 'stan',\n",
       " 'winston',\n",
       " 'design',\n",
       " 'schnazzi',\n",
       " 'cgi',\n",
       " 'occasion',\n",
       " 'shot',\n",
       " 'pick',\n",
       " 'brain',\n",
       " 'if',\n",
       " 'bodi',\n",
       " 'otherwis',\n",
       " 'much',\n",
       " 'sunken',\n",
       " 'jade',\n",
       " 'viewer',\n",
       " 'thank',\n",
       " 'invent',\n",
       " 'timex',\n",
       " 'indiglo',\n",
       " 'base',\n",
       " 'late',\n",
       " '1960',\n",
       " 'televis',\n",
       " 'name',\n",
       " 'mod',\n",
       " 'squad',\n",
       " 'tell',\n",
       " 'tale',\n",
       " 'three',\n",
       " 'reform',\n",
       " 'crimin',\n",
       " 'under',\n",
       " 'employ',\n",
       " 'polic',\n",
       " 'undercov',\n",
       " 'howev',\n",
       " 'wrong',\n",
       " 'evid',\n",
       " 'stolen',\n",
       " 'immedi',\n",
       " 'suspicion',\n",
       " 'ad',\n",
       " 'cut',\n",
       " 'clair',\n",
       " 'dane',\n",
       " 'nice',\n",
       " 'hair',\n",
       " 'cute',\n",
       " 'outfit',\n",
       " 'car',\n",
       " 'stuff',\n",
       " 'blow',\n",
       " 'sound',\n",
       " 'first',\n",
       " 'fifteen',\n",
       " 'becom',\n",
       " 'certain',\n",
       " 'slick',\n",
       " 'costum',\n",
       " 'isn',\n",
       " 'enough',\n",
       " 'best',\n",
       " 'describ',\n",
       " 'cross',\n",
       " 'between',\n",
       " 'hour',\n",
       " 'long',\n",
       " 'cop',\n",
       " 'stretch',\n",
       " 'span',\n",
       " 'singl',\n",
       " 'clich',\n",
       " 'matter',\n",
       " 'element',\n",
       " 'recycl',\n",
       " 'everyth',\n",
       " 'alreadi',\n",
       " 'seen',\n",
       " 'noth',\n",
       " 'spectacular',\n",
       " 'sometim',\n",
       " 'border',\n",
       " 'wooden',\n",
       " 'omar',\n",
       " 'epp',\n",
       " 'deliv',\n",
       " 'their',\n",
       " 'bore',\n",
       " 'transfer',\n",
       " 'onto',\n",
       " 'escap',\n",
       " 'relat',\n",
       " 'unscath',\n",
       " 'giovanni',\n",
       " 'ribisi',\n",
       " 'resid',\n",
       " 'man',\n",
       " 'ultim',\n",
       " 'worth',\n",
       " 'unfortun',\n",
       " 'save',\n",
       " 'convolut',\n",
       " 'apart',\n",
       " 'occupi',\n",
       " 'screen',\n",
       " 'young',\n",
       " 'cast',\n",
       " 'cloth',\n",
       " 'hip',\n",
       " 'soundtrack',\n",
       " 'appear',\n",
       " 'gear',\n",
       " 'toward',\n",
       " 'teenag',\n",
       " 'mindset',\n",
       " 'r',\n",
       " 'rate',\n",
       " 'content',\n",
       " 'justifi',\n",
       " 'juvenil',\n",
       " 'older',\n",
       " 'inform',\n",
       " 'liter',\n",
       " 'spoon',\n",
       " 'hard',\n",
       " 'instead',\n",
       " 'dialogu',\n",
       " 'poor',\n",
       " 'written',\n",
       " 'extrem',\n",
       " 'predict',\n",
       " 'progress',\n",
       " 'won',\n",
       " 'care',\n",
       " 'hero',\n",
       " 'ani',\n",
       " 'jeopardi',\n",
       " 'll',\n",
       " 'aren',\n",
       " 'nobodi',\n",
       " 'rememb',\n",
       " 'question',\n",
       " 'wisdom',\n",
       " 'especi',\n",
       " 'consid',\n",
       " 'target',\n",
       " 'fact',\n",
       " 'number',\n",
       " 'memor',\n",
       " 'can',\n",
       " 'count',\n",
       " 'hand',\n",
       " 'miss',\n",
       " 'finger',\n",
       " 'check',\n",
       " 'six',\n",
       " 'clear',\n",
       " 'indic',\n",
       " 'them',\n",
       " 'than',\n",
       " 'cash',\n",
       " 'spend',\n",
       " 'dollar',\n",
       " 'judg',\n",
       " 'rash',\n",
       " 'aw',\n",
       " 'avoid',\n",
       " 'at',\n",
       " 'cost',\n",
       " 'quest',\n",
       " 'camelot',\n",
       " 'warner',\n",
       " 'bros',\n",
       " 'featur',\n",
       " 'length',\n",
       " 'fulli',\n",
       " 'anim',\n",
       " 'steal',\n",
       " 'clout',\n",
       " 'disney',\n",
       " 'cartoon',\n",
       " 'empir',\n",
       " 'mous',\n",
       " 'reason',\n",
       " 'worri',\n",
       " 'recent',\n",
       " 'challeng',\n",
       " 'throne',\n",
       " 'last',\n",
       " 'fall',\n",
       " 'promis',\n",
       " 'flaw',\n",
       " '20th',\n",
       " 'centuri',\n",
       " 'fox',\n",
       " 'anastasia',\n",
       " 'hercul',\n",
       " 'live',\n",
       " 'color',\n",
       " 'palat',\n",
       " 'had',\n",
       " 'beat',\n",
       " 'crown',\n",
       " '1997',\n",
       " 'piec',\n",
       " 'contest',\n",
       " 'arriv',\n",
       " 'magic',\n",
       " 'kingdom',\n",
       " 'mediocr',\n",
       " '--',\n",
       " 'd',\n",
       " 'pocahonta',\n",
       " 'those',\n",
       " 'keep',\n",
       " 'score',\n",
       " 'near',\n",
       " 'dull',\n",
       " 'revolv',\n",
       " 'adventur',\n",
       " 'free',\n",
       " 'spirit',\n",
       " 'kayley',\n",
       " 'voic',\n",
       " 'jessalyn',\n",
       " 'gilsig',\n",
       " 'earli',\n",
       " 'daughter',\n",
       " 'belat',\n",
       " 'knight',\n",
       " 'king',\n",
       " 'arthur',\n",
       " 'round',\n",
       " 'tabl',\n",
       " 'follow',\n",
       " 'father',\n",
       " 'footstep',\n",
       " 'she',\n",
       " 'chanc',\n",
       " 'evil',\n",
       " 'warlord',\n",
       " 'ruber',\n",
       " 'gari',\n",
       " 'oldman',\n",
       " 'ex',\n",
       " 'gone',\n",
       " 'sword',\n",
       " 'excalibur',\n",
       " 'accident',\n",
       " 'lose',\n",
       " 'danger',\n",
       " 'boobi',\n",
       " 'trap',\n",
       " 'forest',\n",
       " 'help',\n",
       " 'hunki',\n",
       " 'blind',\n",
       " 'timberland',\n",
       " 'dweller',\n",
       " 'garrett',\n",
       " 'carey',\n",
       " 'elw',\n",
       " 'dragon',\n",
       " 'eric',\n",
       " 'idl',\n",
       " 'rickl',\n",
       " 'argu',\n",
       " 'itself',\n",
       " 'abl',\n",
       " 'mediev',\n",
       " 'sexist',\n",
       " 'prove',\n",
       " 'fighter',\n",
       " 'side',\n",
       " 'pure',\n",
       " 'showmanship',\n",
       " 'essenti',\n",
       " 'expect',\n",
       " 'climb',\n",
       " 'high',\n",
       " 'rank',\n",
       " 'differenti',\n",
       " 'someth',\n",
       " 'saturday',\n",
       " 'morn',\n",
       " 'subpar',\n",
       " 'instant',\n",
       " 'forgett',\n",
       " 'song',\n",
       " 'integr',\n",
       " 'computer',\n",
       " 'footag',\n",
       " 'compar',\n",
       " 'angri',\n",
       " 'ogr',\n",
       " 'herc',\n",
       " 'battl',\n",
       " 'hydra',\n",
       " 'rest',\n",
       " 'case',\n",
       " 'stink',\n",
       " 'none',\n",
       " 'remot',\n",
       " 'interest',\n",
       " 'race',\n",
       " 'bland',\n",
       " 'tie',\n",
       " 'win',\n",
       " 'comedi',\n",
       " 'shtick',\n",
       " 'cloy',\n",
       " 'least',\n",
       " 'sign',\n",
       " 'puls',\n",
       " 'fan',\n",
       " \"-'\",\n",
       " '90s',\n",
       " 'tgif',\n",
       " 'will',\n",
       " 'jaleel',\n",
       " 'urkel',\n",
       " 'white',\n",
       " 'bronson',\n",
       " 'balki',\n",
       " 'pinchot',\n",
       " 'share',\n",
       " 'realiz',\n",
       " 'though',\n",
       " 'm',\n",
       " 'loss',\n",
       " 'recal',\n",
       " 'specif',\n",
       " 'provid',\n",
       " 'talent',\n",
       " 'enthusiast',\n",
       " 'pair',\n",
       " 'singer',\n",
       " 'moment',\n",
       " 'jane',\n",
       " 'seymour',\n",
       " 'celin',\n",
       " 'dion',\n",
       " 'must',\n",
       " 'strain',\n",
       " 'through',\n",
       " 'asid',\n",
       " 'children',\n",
       " 'probabl',\n",
       " 'adult',\n",
       " 'grievous',\n",
       " 'error',\n",
       " 'lack',\n",
       " 'learn',\n",
       " 'goe',\n",
       " 'synopsi',\n",
       " 'mental',\n",
       " 'unstabl',\n",
       " 'undergo',\n",
       " 'psychotherapi',\n",
       " 'boy',\n",
       " 'potenti',\n",
       " 'fatal',\n",
       " 'love',\n",
       " 'mother',\n",
       " 'fledgl',\n",
       " 'restauranteur',\n",
       " 'unsuccess',\n",
       " 'gain',\n",
       " 'woman',\n",
       " 'favor',\n",
       " 'take',\n",
       " 'pictur',\n",
       " 'kill',\n",
       " 'comment',\n",
       " 'stalk',\n",
       " 'yet',\n",
       " 'endless',\n",
       " 'string',\n",
       " 'spurn',\n",
       " 'psycho',\n",
       " 'reveng',\n",
       " 'stabl',\n",
       " 'categori',\n",
       " '1990s',\n",
       " 'industri',\n",
       " 'theatric',\n",
       " 'direct',\n",
       " 'prolifer',\n",
       " 'may',\n",
       " 'due',\n",
       " 'typic',\n",
       " 'inexpens',\n",
       " 'produc',\n",
       " 'special',\n",
       " 'effect',\n",
       " 'serv',\n",
       " 'vehicl',\n",
       " 'nuditi',\n",
       " 'allow',\n",
       " 'frequent',\n",
       " 'night',\n",
       " 'cabl',\n",
       " 'waver',\n",
       " 'slight',\n",
       " 'norm',\n",
       " 'respect',\n",
       " 'never',\n",
       " 'affair',\n",
       " ';',\n",
       " 'contrari',\n",
       " 'reject',\n",
       " 'rather',\n",
       " 'lover',\n",
       " 'wife',\n",
       " 'husband',\n",
       " 'entri',\n",
       " 'doom',\n",
       " 'collect',\n",
       " 'dust',\n",
       " 'view',\n",
       " 'midnight',\n",
       " 'suspens',\n",
       " 'set',\n",
       " 'interspers',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'instanc',\n",
       " 'serious',\n",
       " 'narrat',\n",
       " 'spout',\n",
       " 'statist',\n",
       " 'stalker',\n",
       " 'ponder',\n",
       " 'caus',\n",
       " 'implicit',\n",
       " 'impli',\n",
       " 'men',\n",
       " 'shown',\n",
       " 'snapshot',\n",
       " 'jay',\n",
       " 'underwood',\n",
       " 'state',\n",
       " 'daryl',\n",
       " 'gleason',\n",
       " 'brook',\n",
       " 'daniel',\n",
       " 'meant',\n",
       " 'call',\n",
       " 'guesswork',\n",
       " 'requir',\n",
       " 'proceed',\n",
       " 'begin',\n",
       " 'contriv',\n",
       " 'quit',\n",
       " 'victim',\n",
       " 'togeth',\n",
       " 'obsess',\n",
       " 'woo',\n",
       " 'plan',\n",
       " 'desper',\n",
       " 'elabor',\n",
       " 'includ',\n",
       " 'murder',\n",
       " 'pet',\n",
       " 'found',\n",
       " 'except',\n",
       " 'cat',\n",
       " 'shower',\n",
       " 'event',\n",
       " 'lead',\n",
       " 'inevit',\n",
       " 'showdown',\n",
       " 'surviv',\n",
       " 'invari',\n",
       " 'conclus',\n",
       " 'turkey',\n",
       " 'uniform',\n",
       " 'adequ',\n",
       " 'anyth',\n",
       " 'home',\n",
       " 'either',\n",
       " 'melodrama',\n",
       " 'overdo',\n",
       " 'word',\n",
       " 'manag',\n",
       " 'creepi',\n",
       " 'pass',\n",
       " 'demand',\n",
       " 'maryam',\n",
       " 'abo',\n",
       " 'close',\n",
       " 'bond',\n",
       " 'chick',\n",
       " 'daylight',\n",
       " 'equal',\n",
       " 'titl',\n",
       " 'ditzi',\n",
       " 'strong',\n",
       " 'independ',\n",
       " 'busi',\n",
       " 'owner',\n",
       " 'exampl',\n",
       " 'ensur',\n",
       " 'use',\n",
       " 'excus',\n",
       " 'return',\n",
       " 'toolbox',\n",
       " 'left',\n",
       " 'place',\n",
       " 'hous',\n",
       " 'leav',\n",
       " 'door',\n",
       " 'answer',\n",
       " 'wander',\n",
       " 'our',\n",
       " 'heroin',\n",
       " 'somehow',\n",
       " 'park',\n",
       " 'front',\n",
       " 'right',\n",
       " 'oblivi',\n",
       " 'presenc',\n",
       " 'insid',\n",
       " 'whole',\n",
       " 'episod',\n",
       " 'incred',\n",
       " 'disbelief',\n",
       " 'valid',\n",
       " 'intellig',\n",
       " 'receiv',\n",
       " 'deriv',\n",
       " 'somewhat',\n",
       " 'cannot',\n",
       " 'sever',\n",
       " 'brief',\n",
       " 'strip',\n",
       " 'bar',\n",
       " 'offens',\n",
       " 'mani',\n",
       " 'thriller',\n",
       " 'mood',\n",
       " 'stake',\n",
       " 'els',\n",
       " 'capsul',\n",
       " '2176',\n",
       " 'planet',\n",
       " 'mar',\n",
       " 'custodi',\n",
       " 'accus',\n",
       " 'face',\n",
       " 'menac',\n",
       " 'lot',\n",
       " 'fight',\n",
       " 'john',\n",
       " 'carpent',\n",
       " 'repris',\n",
       " 'previous',\n",
       " 'assault',\n",
       " 'precinct',\n",
       " '13',\n",
       " 'homag',\n",
       " 'himself',\n",
       " '0',\n",
       " '+',\n",
       " 'believ',\n",
       " 'horribl',\n",
       " 'writer',\n",
       " 'suppos',\n",
       " 'expert',\n",
       " 'mistak',\n",
       " 'ghost',\n",
       " 'drawn',\n",
       " 'human',\n",
       " 'surpris',\n",
       " 'low',\n",
       " 'alien',\n",
       " 'addit',\n",
       " 'anybodi',\n",
       " 'made',\n",
       " 'ground',\n",
       " 'sue',\n",
       " 'chock',\n",
       " 'full',\n",
       " 'princ',\n",
       " 'dark',\n",
       " 'fit',\n",
       " 'admit',\n",
       " 'novel',\n",
       " 'scienc',\n",
       " 'fiction',\n",
       " 'experi',\n",
       " 'terraform',\n",
       " 'walk',\n",
       " 'surfac',\n",
       " 'without',\n",
       " 'breath',\n",
       " 'budget',\n",
       " 'mention',\n",
       " 'graviti',\n",
       " 'increas',\n",
       " 'earth',\n",
       " 'easier',\n",
       " 'societi',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add Arabic Lexicon words\n",
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "with open('NileULex2.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    lexicon = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in lexicon:\n",
    "        clas = row[1]\n",
    "        row = row[0].split()\n",
    "        if clas == '1':\n",
    "            for r in row:\n",
    "                pos_words.extend([r])\n",
    "        else:\n",
    "            for r in row:\n",
    "                neg_words.extend([r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add English Lexicon words\n",
    "with open('Hu and Liu Lexicon.csv', newline='', encoding='latin-1') as csvfile:\n",
    "    lexicon = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in lexicon:\n",
    "        clas = row[1]\n",
    "        row = row[0].split()\n",
    "        if clas == '1':\n",
    "            for r in row:\n",
    "                pos_words.extend([r.strip()])\n",
    "        else:\n",
    "            for r in row:\n",
    "                neg_words.extend([r.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read negation words\n",
    "negation_words = []\n",
    "with open('negation_words.txt', encoding='utf-8') as f:\n",
    "    negation_words = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Word Features:  9920\n"
     ]
    }
   ],
   "source": [
    "#read stop words\n",
    "with open('stop_words.txt', encoding='utf-8') as f:\n",
    "    stop_words = f.read().splitlines() \n",
    "word_features2 = []\n",
    "for w in word_features:\n",
    "    if not w in stop_words:\n",
    "        word_features2.append(w)\n",
    "print('Total Word Features: ', len(word_features2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_formatted))\n",
    "feature_set = []\n",
    "feature_set2 = []\n",
    "class_set = []\n",
    "j = 0\n",
    "for (comment, clas) in dataset_formatted:\n",
    "    j += 1\n",
    "    words = list(comment)\n",
    "    feat = {}\n",
    "    features = []\n",
    "    for w in word_features2:\n",
    "        flag = w in words\n",
    "        if flag:\n",
    "            features.extend([1])\n",
    "        else:\n",
    "            features.extend([0])\n",
    "        feat[w] = flag\n",
    "        \n",
    "    feature_set.append(features)\n",
    "    feature_set2.append([feat, clas])\n",
    "    class_set.extend([clas])\n",
    "print(len(dataset_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 ,  9920\n"
     ]
    }
   ],
   "source": [
    "M = len(feature_set)\n",
    "N = len(feature_set[0])\n",
    "print(M, ', ', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DNN Classifier\n",
    "slic = int(M*0.1)\n",
    "testing_set = feature_set[:slic]\n",
    "class_testing_set = class_set[:slic]\n",
    "training_set = feature_set\n",
    "class_training_set = class_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 200\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,), random_state=1)\n",
    "clf.fit(training_set, class_training_set)\n",
    "output = clf.predict(testing_set)\n",
    "wrong = sum(i != j for i, j in zip(output, class_testing_set))\n",
    "acc = (len(output) - wrong)/len(output)\n",
    "print(wrong, len(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "with open('dnn_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid, protocol=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output\n",
    "class_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = len(feature_set2)\n",
    "slic = int(M*0.1)\n",
    "testing_set2 = feature_set2[:slic]\n",
    "training_set2 = feature_set2[slic:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#NaiveBayes Classifier \n",
    "clf = nltk.NaiveBayesClassifier.train(training_set2)\n",
    "nltk.classify.accuracy(clf, testing_set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classify a comment\n",
    "def comm_classifier(comm):\n",
    "    #comm = 'الراجل ده برنس و بيقول كلام زى الفل'\n",
    "    comm = 'He is a bad actor and I think he was like a fool in the last movie'\n",
    "    \n",
    "    sum_words = 1e-9\n",
    "    cn_value = 0\n",
    "    neg = 1\n",
    "    comm = comm.split()\n",
    "    for w in comm:\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "        elif w in negation_words:\n",
    "            neg *= -1\n",
    "            continue\n",
    "        if w in pos_words:\n",
    "            print('POS: ', w)\n",
    "            cn_value += (neg * 1)\n",
    "            sum_words += 1\n",
    "        if w in neg_words:\n",
    "            print('NEG: ', w)\n",
    "            cn_value += (neg * -1)\n",
    "            sum_words += 1\n",
    "    \n",
    "    val = cn_value/sum_words\n",
    "    if val > 0.3:\n",
    "        val = 1\n",
    "    elif val < -0.3:\n",
    "        val = -1\n",
    "    elif val > -0.1 and val < 0.1:\n",
    "        val = 0\n",
    "    #else:\n",
    "     #   val = clf.predict(comm)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comm_classifier('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "print(translator.translate('dh a7sn brnamg shofto fe 7yate', src='ar', dest='ar').text)\n",
    "#print(translator.detect('dh a7sn brnamg shofto fe 7eate'))\n",
    "\n",
    "from langdetect import detect\n",
    "#print(detect(\"dh a7sn brnamg shofto fe 7eate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('english_features', 'wb') as fp:\n",
    "    pickle.dump(word_features2, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9920"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
